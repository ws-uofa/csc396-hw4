{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a21f6c-022c-4c66-84a9-1eedfbdb3b95",
   "metadata": {},
   "source": [
    "### Imports and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452c7aeb-0f3f-473d-a2cf-3d60fcaa7534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Model: GRID A100X-10C\n",
      "Total VRAM: 10.00 GB\n",
      "Loading microsoft/deberta-v3-base...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2Model(\n",
       "  (embeddings): DebertaV2Embeddings(\n",
       "    (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): DebertaV2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x DebertaV2Layer(\n",
       "        (attention): DebertaV2Attention(\n",
       "          (self): DisentangledSelfAttention(\n",
       "            (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): DebertaV2SelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): DebertaV2Intermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): DebertaV2Output(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rel_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name).half().to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7981e7d-d2e9-47ba-b77a-b628dda3b012",
   "metadata": {},
   "source": [
    "### Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d75ac470-0898-41a4-a8b5-934846751b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid sentences: 3980290\n",
      "Starting processing with Batch Size = 64...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|█████████████████| 62193/62193 [36:45<00:00, 28.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Complete. Unique tokens found: 121122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = \"assignment4-dataset.txt\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "token_embedding_sums = defaultdict(lambda: torch.zeros(model.config.hidden_size, dtype=torch.float32))\n",
    "token_counts = defaultdict(int)\n",
    "token_id_to_word = {}\n",
    "\n",
    "\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    all_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "total_lines = len(all_lines)\n",
    "print(f\"Total valid sentences: {total_lines}\")\n",
    "\n",
    "\n",
    "def run_processing():\n",
    "    print(f\"Starting processing with Batch Size = {BATCH_SIZE}...\")\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(0, total_lines, BATCH_SIZE), desc=\"Processing Batches\"):\n",
    "        # Batching (with padding)\n",
    "        batch_lines = all_lines[i : i + BATCH_SIZE]\n",
    "\n",
    "        # Tokenization\n",
    "        inputs = tokenizer(batch_lines, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "\n",
    "        # GPU Inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        embeddings = outputs.last_hidden_state\n",
    "        \n",
    "        # Back to CPU (with masking)\n",
    "        input_ids_cpu = input_ids.cpu().numpy()\n",
    "        attention_mask_cpu = attention_mask.cpu().numpy()\n",
    "        embeddings_cpu = embeddings.float().cpu()\n",
    "\n",
    "\n",
    "        # Sum tokens' embeddings\n",
    "        batch_len = len(batch_lines)\n",
    "        for b_idx in range(batch_len):\n",
    "            valid_len = np.sum(attention_mask_cpu[b_idx])\n",
    "            \n",
    "            curr_ids = input_ids_cpu[b_idx][:valid_len]\n",
    "            curr_embs = embeddings_cpu[b_idx][:valid_len]\n",
    "            \n",
    "            for t_idx, token_id in enumerate(curr_ids):\n",
    "                if token_id not in token_id_to_word:\n",
    "                    token_id_to_word[token_id] = tokenizer.decode([token_id])\n",
    "                \n",
    "                token_embedding_sums[token_id] += curr_embs[t_idx]\n",
    "                token_counts[token_id] += 1                \n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "run_processing()\n",
    "print(f\"Processing Complete. Unique tokens found: {len(token_embedding_sums)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1233443-9a0b-4b7c-b1ae-b56974045fb2",
   "metadata": {},
   "source": [
    "### Compute Averages and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2596ed63-ad33-4743-8929-4befe3da0061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          | [CLS]           | 3980290  | [768]\n",
      "2          | [SEP]           | 3980290  | [768]\n",
      "132        | �               | 94       | [768]\n",
      "133        | �               | 142      | [768]\n",
      "134        | �               | 144      | [768]\n",
      "Saved to average_token_embeddings.pt\n"
     ]
    }
   ],
   "source": [
    "average_embeddings = {}\n",
    "\n",
    "for token_id, vector_sum in token_embedding_sums.items():\n",
    "    count = token_counts[token_id]\n",
    "    average_embeddings[token_id] = vector_sum / count\n",
    "\n",
    "sorted_ids = sorted(list(average_embeddings.keys()))[:5]\n",
    "\n",
    "for tid in sorted_ids:\n",
    "    text = token_id_to_word[tid]\n",
    "    cnt = token_counts[tid]\n",
    "    shape = list(average_embeddings[tid].shape)\n",
    "    print(f\"{tid:<10} | {text:<15} | {cnt:<8} | {shape}\")\n",
    "\n",
    "torch.save(average_embeddings, \"average_token_embeddings.pt\")\n",
    "print(\"Saved to average_token_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454862f2-0e34-465a-9bfa-6b7d9cc08f94",
   "metadata": {},
   "source": [
    "### Tokens to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ef5352-77b6-4e83-82d7-6a6fd33461a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 words in vocabulary.\n",
      "Created embeddings for 398937 words.\n",
      "Missing sub-tokens encountered: 2319\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "glove_file = \"glove.6B.300d-vocabulary.txt\"\n",
    "with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    glove_words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Found {len(glove_words)} words in vocabulary.\")\n",
    "\n",
    "# Token Embeddings -> Word Embeddings\n",
    "word_embeddings = {}\n",
    "missing_tokens_count = 0\n",
    "\n",
    "avg_emb_cpu = {k: v.cpu() for k, v in average_embeddings.items()}\n",
    "\n",
    "for word in glove_words:\n",
    "    token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "    \n",
    "    vectors = []\n",
    "    for tid in token_ids:\n",
    "        if tid in avg_emb_cpu:\n",
    "            vectors.append(avg_emb_cpu[tid])\n",
    "        else:\n",
    "            missing_tokens_count += 1\n",
    "    \n",
    "    if vectors:\n",
    "        word_vec = torch.stack(vectors).mean(dim=0)\n",
    "        word_embeddings[word] = word_vec\n",
    "\n",
    "print(f\"Created embeddings for {len(word_embeddings)} words.\")\n",
    "print(f\"Missing sub-tokens encountered: {missing_tokens_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af61f9c-701b-4171-90d9-d2b241ec5ee1",
   "metadata": {},
   "source": [
    "### most_similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adae3503-9894-49c5-bb1b-b10617bc681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "vocab_list = list(word_embeddings.keys())\n",
    "\n",
    "key_to_index = {word: i for i, word in enumerate(vocab_list)}\n",
    "index_to_key = {i: word for i, word in enumerate(vocab_list)}\n",
    "\n",
    "vectors_tensor = torch.stack([word_embeddings[w] for w in vocab_list])\n",
    "\n",
    "vectors_normalized = F.normalize(vectors_tensor, p=2, dim=1)\n",
    "\n",
    "\n",
    "def most_similar(word, vectors, index_to_key, key_to_index, topn=10):\n",
    "    word_id = key_to_index[word]\n",
    "    emb = vectors[word_id]\n",
    "    similarities = torch.matmul(vectors, emb)\n",
    "    ids_ascending = torch.argsort(similarities)\n",
    "    ids_descending = torch.flip(ids_ascending, dims=[0])\n",
    "    mask = ids_descending != word_id\n",
    "    ids_descending = ids_descending[mask]\n",
    "    top_ids = ids_descending[:topn]\n",
    "    top_words = []\n",
    "    for i in top_ids:\n",
    "        idx = i.item()\n",
    "        word_text = index_to_key[idx]\n",
    "        score = similarities[idx].item()\n",
    "        top_words.append((word_text, score))\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb8f14-aa08-4702-894a-e7f65ed2008d",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cb21fff-a5ce-44d6-856d-6cc6902a9232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: king\n",
      "  queen           : 0.9901\n",
      "  emperor         : 0.9881\n",
      "  bishop          : 0.9854\n",
      "  kings           : 0.9810\n",
      "  churchwomen     : 0.9785\n",
      "------------------------------\n",
      "Query: computer\n",
      "  medical         : 0.9915\n",
      "  computerworld   : 0.9893\n",
      "  animal          : 0.9884\n",
      "  newsradio       : 0.9879\n",
      "  chemical        : 0.9872\n",
      "------------------------------\n",
      "Query: science\n",
      "  oil             : 0.9870\n",
      "  ironworking     : 0.9870\n",
      "  blackmarket     : 0.9862\n",
      "  brainpop        : 0.9857\n",
      "  fishmarket      : 0.9856\n",
      "------------------------------\n",
      "Query: university\n",
      "  cathedral       : 0.9860\n",
      "  railroad        : 0.9830\n",
      "  army            : 0.9819\n",
      "  church          : 0.9817\n",
      "  temple          : 0.9804\n",
      "------------------------------\n",
      "Query: cactus\n",
      "  tuna            : 0.9890\n",
      "  fern            : 0.9886\n",
      "  squid           : 0.9871\n",
      "  crocodile       : 0.9865\n",
      "  jellyfish       : 0.9862\n",
      "------------------------------\n",
      "Query: happy\n",
      "  angry           : 0.9800\n",
      "  popular         : 0.9782\n",
      "  useful          : 0.9776\n",
      "  excited         : 0.9773\n",
      "  rich            : 0.9773\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_words = [\"king\", \"computer\", \"science\", \"university\",\"cactus\", \"happy\"]\n",
    "\n",
    "for w in test_words:\n",
    "    print(f\"Query: {w}\")\n",
    "    results = most_similar(w, vectors_normalized, index_to_key, key_to_index, topn=5)\n",
    "    \n",
    "    if isinstance(results, str):\n",
    "        print(f\"  {results}\")\n",
    "    else:\n",
    "        for match_word, score in results:\n",
    "            print(f\"  {match_word:<15} : {score:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e713fc2-2c62-44f5-affb-6c34e88dc460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
