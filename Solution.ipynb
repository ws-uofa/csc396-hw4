{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a21f6c-022c-4c66-84a9-1eedfbdb3b95",
   "metadata": {},
   "source": [
    "### Imports and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c7aeb-0f3f-473d-a2cf-3d60fcaa7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name).half().to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7981e7d-d2e9-47ba-b77a-b628dda3b012",
   "metadata": {},
   "source": [
    "### Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ac470-0898-41a4-a8b5-934846751b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"assignment4-dataset.txt\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "token_embedding_sums = defaultdict(lambda: torch.zeros(model.config.hidden_size, dtype=torch.float32))\n",
    "token_counts = defaultdict(int)\n",
    "token_id_to_word = {}\n",
    "\n",
    "\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    all_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "total_lines = len(all_lines)\n",
    "print(f\"Total valid sentences: {total_lines}\")\n",
    "\n",
    "\n",
    "def run_processing():\n",
    "    print(f\"Starting processing with Batch Size = {BATCH_SIZE}...\")\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(0, total_lines, BATCH_SIZE), desc=\"Processing Batches\"):\n",
    "        # Batching (with padding)\n",
    "        batch_lines = all_lines[i : i + BATCH_SIZE]\n",
    "\n",
    "        # Tokenization\n",
    "        inputs = tokenizer(batch_lines, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "\n",
    "        # GPU Inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        embeddings = outputs.last_hidden_state\n",
    "        \n",
    "        # Back to CPU (with masking)\n",
    "        input_ids_cpu = input_ids.cpu().numpy()\n",
    "        attention_mask_cpu = attention_mask.cpu().numpy()\n",
    "        embeddings_cpu = embeddings.float().cpu()\n",
    "\n",
    "\n",
    "        # Sum tokens' embeddings\n",
    "        batch_len = len(batch_lines)\n",
    "        for b_idx in range(batch_len):\n",
    "            valid_len = np.sum(attention_mask_cpu[b_idx])\n",
    "            \n",
    "            curr_ids = input_ids_cpu[b_idx][:valid_len]\n",
    "            curr_embs = embeddings_cpu[b_idx][:valid_len]\n",
    "            \n",
    "            for t_idx, token_id in enumerate(curr_ids):\n",
    "                if token_id not in token_id_to_word:\n",
    "                    token_id_to_word[token_id] = tokenizer.decode([token_id])\n",
    "                \n",
    "                token_embedding_sums[token_id] += curr_embs[t_idx]\n",
    "                token_counts[token_id] += 1                \n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "run_processing()\n",
    "print(f\"Processing Complete. Unique tokens found: {len(token_embedding_sums)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1233443-9a0b-4b7c-b1ae-b56974045fb2",
   "metadata": {},
   "source": [
    "### Compute Averages and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596ed63-ad33-4743-8929-4befe3da0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_embeddings = {}\n",
    "\n",
    "for token_id, vector_sum in token_embedding_sums.items():\n",
    "    count = token_counts[token_id]\n",
    "    average_embeddings[token_id] = vector_sum / count\n",
    "\n",
    "sorted_ids = sorted(list(average_embeddings.keys()))[:5]\n",
    "\n",
    "for tid in sorted_ids:\n",
    "    text = token_id_to_word[tid]\n",
    "    cnt = token_counts[tid]\n",
    "    shape = list(average_embeddings[tid].shape)\n",
    "    print(f\"{tid:<10} | {text:<15} | {cnt:<8} | {shape}\")\n",
    "\n",
    "torch.save(average_embeddings, \"average_token_embeddings.pt\")\n",
    "print(\"Saved to average_token_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454862f2-0e34-465a-9bfa-6b7d9cc08f94",
   "metadata": {},
   "source": [
    "### Tokens to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef5352-77b6-4e83-82d7-6a6fd33461a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "glove_file = \"glove.6B.300d-vocabulary.txt\"\n",
    "with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    glove_words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Found {len(glove_words)} words in vocabulary.\")\n",
    "\n",
    "# Token Embeddings -> Word Embeddings\n",
    "word_embeddings = {}\n",
    "missing_tokens_count = 0\n",
    "\n",
    "avg_emb_cpu = {k: v.cpu() for k, v in average_embeddings.items()}\n",
    "\n",
    "for word in glove_words:\n",
    "    token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "    \n",
    "    vectors = []\n",
    "    for tid in token_ids:\n",
    "        if tid in avg_emb_cpu:\n",
    "            vectors.append(avg_emb_cpu[tid])\n",
    "        else:\n",
    "            missing_tokens_count += 1\n",
    "    \n",
    "    if vectors:\n",
    "        word_vec = torch.stack(vectors).mean(dim=0)\n",
    "        word_embeddings[word] = word_vec\n",
    "\n",
    "print(f\"Created embeddings for {len(word_embeddings)} words.\")\n",
    "print(f\"Missing sub-tokens encountered: {missing_tokens_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af61f9c-701b-4171-90d9-d2b241ec5ee1",
   "metadata": {},
   "source": [
    "### most_similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae3503-9894-49c5-bb1b-b10617bc681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "vocab_list = list(word_embeddings.keys())\n",
    "\n",
    "key_to_index = {word: i for i, word in enumerate(vocab_list)}\n",
    "index_to_key = {i: word for i, word in enumerate(vocab_list)}\n",
    "\n",
    "vectors_tensor = torch.stack([word_embeddings[w] for w in vocab_list])\n",
    "\n",
    "vectors_normalized = F.normalize(vectors_tensor, p=2, dim=1)\n",
    "\n",
    "\n",
    "def most_similar(word, vectors, index_to_key, key_to_index, topn=10):\n",
    "    word_id = key_to_index[word]\n",
    "    emb = vectors[word_id]\n",
    "    similarities = torch.matmul(vectors, emb)\n",
    "    ids_ascending = torch.argsort(similarities)\n",
    "    ids_descending = torch.flip(ids_ascending, dims=[0])\n",
    "    mask = ids_descending != word_id\n",
    "    ids_descending = ids_descending[mask]\n",
    "    top_ids = ids_descending[:topn]\n",
    "    top_words = []\n",
    "    for i in top_ids:\n",
    "        idx = i.item()\n",
    "        word_text = index_to_key[idx]\n",
    "        score = similarities[idx].item()\n",
    "        top_words.append((word_text, score))\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb8f14-aa08-4702-894a-e7f65ed2008d",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb21fff-a5ce-44d6-856d-6cc6902a9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [\"king\", \"computer\", \"science\", \"arizona\",\"cactus\" \"happy\"]\n",
    "\n",
    "for w in test_words:\n",
    "    print(f\"Query: {w}\")\n",
    "    results = most_similar_words(w, vectors_normalized, index_to_key, key_to_index, topn=5)\n",
    "    \n",
    "    if isinstance(results, str):\n",
    "        print(f\"  {results}\")\n",
    "    else:\n",
    "        for match_word, score in results:\n",
    "            print(f\"  {match_word:<15} : {score:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
